# Machine Learning Inference Engine Library

## Overview

The MTB-ML Inference Engine is a library which deploys an ML model and performs complete inference in different numerical formats on PSoC6 embedded devices.

## Requirements

* [ModusToolbox® software](https://www.cypress.com/products/modustoolbox-software-environment) v2.2
* Board Support Package (BSP) minimum required version: 2.0.0
* Programming Language: C
* Associated Parts: See "Supported Kits" section below.

## Supported Toolchains (make variable 'TOOLCHAIN')

* GNU Arm® Embedded Compiler v9.3.1 (`GCC_ARM`)
* Arm compiler v6.13 (`ARM`)
* IAR C/C++ compiler v8.42.1 (`IAR`)

## Supported Kits (make variable 'TARGET')

* [PSoC 6 Wi-Fi BT Prototyping Kit (CY8CPROTO-062-4343W)](https://www.cypress.com/documentation/development-kitsboards/psoc-6-wi-fi-bt-prototyping-kit-cy8cproto-062-4343w)
* [PSoC® 64 Standard Secure - AWS Wi-Fi BT Pioneer Kit (CY8CKIT-064S0S2-4343W)](https://www.cypress.com/documentation/development-kitsboards/psoc-64-standard-secure-aws-wi-fi-bt-pioneer-kit-cy8ckit)

## Features

Supports the following typical ML models:
* MLP - Dense feed-forward network
* CONV1D
* CONV2D
* RNNs - GRU

Supports floating-point and fixed-point variants:
* 32-bit floating-point
* 16-bit fixed-point input
* 8-bit fixed-point input
* 16-bit fixed-point weight
* 8-bit fixed-point weight
* Support model, layer, and frame profiling

## Quick Start

### Adding the library

You can add a dependency file (MTB format) under the deps folder or use the Library Manager to add it in your project. It is available under Library > MCU Middleware > ml.

In the Makefile of the project, you need to define the quantization to be deployed. Note that you can only choose one the type of quantization. In the COMPONENTS parameter, add one of the following:
* ML_FLOAT32: use 32-bit floating-point for the weights and input data
* ML_INT16x16: use 16-bit fixed-point for the weights and input data
* ML_INT16x8: use 16-bit fixed-point for the input data and 8-bit for the weights
* ML_INT8x16: use 8-bit fixed-point for the input data and 16-bit for the weights
* ML_INT8x8: use 8-bit fixed-point for the weights and input data

### Using the library

There are four steps to use the ML-inference engine library.

#### Step 1: Get required memory for the inference engine

One of the data arrays generated by the ML Configurator Tool contains the model parameters. If using binary file, it is placed in the *NN_model_prms.bin*. If using C array header, it is placed in the *NN_model_all.h* > NN_model_prms_bin[].

Use the `Cy_ML_Model_Parse()` function to extract information from the data. It tells how much memory is required by the persistent and scratch memory.

#### Step 2: Allocate Memory

Once you know how much memory is required by the persistent and scratch memory, allocate it in your application. Here is an example in C on how to perform Step 1 and 2.

```c
#include NN_model_all.h
...
cy_stc_ml_model_info_t model_xx_info;

count = Cy_ML_Model_Parse(NN_model_prms_bin, &model_xx_info);
if (count > 0) /* Model parsing successful */
{
    persistent_mem = (char*) malloc(model_xx_info.persistent_mem*sizeof(char));
    scratch_mem = (char*) malloc(model_xx_info.scratch_mem*sizeof(char));
}
```

#### Step 3: Initialize Model and get Model Container/Object

After allocating memory, you can initialize the interference engine by providing pointers to the memory and the model weights. Here is an example in C using float quantization:

```c
#include NN_model_all.h
...
void *model_xx_obj;

result = Cy_ML_Model_Init(&model_xx_obj,      // NN model data container pointer
                          &NN_model_flt_bin,  // NN model parameter buffer pointer
                          persistent_mem,     // Pointer to allocated persistent mem
                          scratch_mem,        // Pointer to allocated scratch mem
                          &model_xx_info,     // Pointer to model info structure
                          1,100);             // Recurrent NN reset flag and window size
```

#### Step 4: Run the inference engine

The last step is to run the inference engine. In this step, the input data can come from sensors or from the regression data. Here is an example in C using float quantization.

```c
result = Cy_ML_Model_Inference(&model_xx_obj, // NN model data container pointer
                               in_buffer,     // Input buffer
                               out_buffer,    // Output buffer
                               NULL,          // Not used in floating-point
                               0);            // Not used in floating-point
```

Note that the last two arguments of the function above is only used in fixed-point. The second last argument is the pointer to input data and output data fixed-point Q factor. And the last argument is the input data sample size.

### Verifying the inference

The ML-inference engine library integrates a profiling mechanism to obtain the number of cycles the library takes to execute. To make this work, the application needs to implement the following function:

```c
int Cy_ML_Profile_Get_Tsc(uint32_t *val)
```

This function needs to return a counter value that increments on every CPU cycle. One way to implement this is to run a timer continuously that triggers an interrupt every second to increment a variable to represent how many seconds passed. On calling `Cy_ML_Profile_Get_Tsc`, it gets the number of seconds passed and combine with the current counter. Then it translates the result in CPU cycles.

To actually enable the profiling from the ML-inference engine library, a few functions need to be called. Here is the
flow:

1. Call `Cy_ML_Profile_Init()` to initialize the profiling or disable all profiling features enabled.
2. Call `Cy_ML_Profile_Start()` before executing `Cy_ML_Model_Inference()`.
3. Call `Cy_ML_Profile_Update()` after executing `Cy_ML_Model_Inference()`.
4. Call `Cy_ML_Profile_Close()` after feeding all the regression data.

An example using the regression data generated by the ML Configurator tool and the profiling integrated in the ML-inference engine library is available in this link:
https://github.com/cypresssemiconductorco/mtb-example-ml-profiler

There are few options of profiling information printed when enabled, as shown in the following list:

| Funtion to call                    | Description |
| :---                               | :----       |
| `Cy_ML_Enable_Layer_Profile()`     | Enable/Disable Layer Profiling. It prints general information of each layer of the NN model, such as average cycle, peak cycle and peak frame. |
| `Cy_ML_Enable_Model_Output()`      | Enable/Disable the output profiling. It prints the output values generated by the inference engine. |
| `Cy_ML_Enable_Model_Profile()`     | Enable/Disable the NN model profiling. It prints general information about the NN model, such as average cycle, peak cycle and peak frame. |
| `Cy_ML_Enable_Per_Frame_Profile()` | Enable/Disable the per frame profiling. It prints general information for each frame, such as number of cycles per frame. |
